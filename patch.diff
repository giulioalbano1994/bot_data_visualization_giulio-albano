*** Begin Patch
*** Update File: modules/llm_processor.py
@@
-class LLMProcessor:
-    """
-    Modulare: build_prompt ��' call_llm ��' parse_response ��' normalize/fallback.
-    Mantiene i metodi pubblici usati da main.py:
-    - available_variables(limit)
-    - process_request(text) -> QueryParameters
-    - generate_commentary(df, params) -> str
-    """
+class LLMProcessor:
+    """
+    Modulare: build_prompt -> call_llm -> parse_response -> normalize/fallback.
+    Mantiene i metodi pubblici usati da main.py:
+    - available_variables(limit)
+    - process_request(text) -> QueryParameters
+    - generate_commentary(df, params) -> str
+    """
@@
-            system = (
-                "Sei un assistente di data analysis. Ricevi un CSV di preview (gi�� aggregato). "
-                "Rispondi con 3�?"6 punti elenco (ognuno inizia con '- ') in italiano: livelli, differenze comuni, trend, 1�?"2 insight. "
-                "Niente premesse, niente codice, niente markdown extra."
-            )
+            system = (
+                "Sei un assistente di data analysis. Ricevi un CSV di preview (gia' aggregato). "
+                "Rispondi con 3-6 punti elenco (ognuno inizia con '- ') in italiano: livelli, differenze tra comuni, trend, 1-2 insight. "
+                "Niente premesse, niente codice, niente markdown extra."
+            )
@@
-    def _build_prompt(self, user_request: str) -> (str, str):
+    def _build_prompt(self, user_request: str) -> (str, str):
@@
-        system = f"""
+        system = f"""
 Sei un planner che converte richieste in linguaggio naturale in un JSON **valido** per una query dati.
 Regole:
 - Usa SOLO i nomi di colonne disponibili nel dataset.
-- Imposta 'query_type' e 'chart_type' (preferenza: serie storiche ��' chart=line).
+- Imposta 'query_type' e 'chart_type' (preferenza: serie storiche -> chart=line).
 - 'metrics' deve contenere nomi canonici (es: pop_totale, average_income, total_income, gini_index).
 - Consenti 'derived_metrics' con campi: name, formula OPPURE (type in ['per_capita','share_of'] con metric/base o num/den).
-- Se non �� specificato il periodo e NON �� una time series, l'anno sar�� deciso a valle (ultimo disponibile).
+- Se non e' specificato il periodo e NON e' una time series, l'anno sara' deciso a valle (ultimo disponibile).
 - Rispondi **solo** con JSON (senza testo extra).
@@
-        if any(k in t for k in keywords_percent):
-            return True, True
-        if any(k in t for k in keywords_ratio):
-            return True, False
+        if any(k in t for k in keywords_percent):
+            return True, True
+        if any(k in t for k in keywords_ratio):
+            return True, False
         return None, None
 
-# ---------- Internals: Catalog & Mappings ----------
+    # ---------- Internals: Catalog & Mappings ----------
     def _load_variable_catalog(self) -> pd.DataFrame:
-        """Carica il catalogo variabili. Se non trova dizionario_variabili.csv,
-        legge direttamente le colonne dal dataset principale (resources/df_ridotto_bot.csv o .xlsx)."""
+        """Carica il catalogo variabili. Se non trova dizionario_variabili.csv,
+        legge direttamente le colonne dal dataset principale (resources/df_ridotto_bot.csv o .xlsx)."""
         import os
         import pandas as pd
 
         cands = [
-        os.getenv("VARIABLES_DICT", "").strip(),
-        os.path.join(os.getcwd(), "resources", "dizionario_variabili.csv"),
-        os.path.join(os.getcwd(), "dizionario_variabili.csv"),
+            os.getenv("VARIABLES_DICT", "").strip(),
+            os.path.join(os.getcwd(), "resources", "dizionario_variabili.csv"),
+            os.path.join(os.getcwd(), "dizionario_variabili.csv"),
         ]
 
-        for p in cands:
-            if p and os.path.exists(p):
-                try:
-                    df = pd.read_csv(p)
-                    except Exception:
-                df = pd.read_csv(p, sep=";")
-            df.columns = [c.strip().lower() for c in df.columns]
-            for c in df.columns:
-                if df[c].dtype == "object":
-                    df[c] = df[c].astype(str).str.strip()
-            print(f"�o. Catalogo variabili caricato da file: {p}")
-            return df
+        for p in cands:
+            if p and os.path.exists(p):
+                try:
+                    df = pd.read_csv(p)
+                except Exception:
+                    df = pd.read_csv(p, sep=';')
+                df.columns = [c.strip().lower() for c in df.columns]
+                for c in df.columns:
+                    if df[c].dtype == "object":
+                        df[c] = df[c].astype(str).str.strip()
+                print(f"OK. Catalogo variabili caricato da file: {p}")
+                return df
 
         # Fallback: genera il catalogo direttamente dal dataset principale
-        dataset_candidates = [
-        os.path.join("resources", "df_ridotto_bot.csv"),
-        os.path.join("resources", "df_ridotto_bot.xlsx"),
-        ]
+        dataset_candidates = [
+            os.path.join("resources", "df_ridotto_bot.csv"),
+            os.path.join("resources", "df_ridotto_bot.xlsx"),
+        ]
 
-        for d in dataset_candidates:
-        if os.path.exists(d):
-            try:
-                if d.endswith(".csv"):
-                    cols = pd.read_csv(d, nrows=1).columns.tolist()
-                else:
-                    cols = pd.read_excel(d, nrows=1).columns.tolist()
-                df = pd.DataFrame({
-                    "variabile": cols,
-                    "descrizione": ["" for _ in cols],
-                    "sinonimi/termini collegati": ["" for _ in cols],
-                    "livello": ["comunale" for _ in cols],
-                })
-                print(f"�o. Catalogo variabili generato dal dataset principale ({len(cols)} colonne)")
-                return df
-            except Exception as e:
-                print(f"�?O Errore durante la lettura del dataset per il catalogo: {e}")
+        for d in dataset_candidates:
+            if os.path.exists(d):
+                try:
+                    if d.endswith(".csv"):
+                        cols = pd.read_csv(d, nrows=1).columns.tolist()
+                    else:
+                        cols = pd.read_excel(d, nrows=1).columns.tolist()
+                    df = pd.DataFrame({
+                        "variabile": cols,
+                        "descrizione": ["" for _ in cols],
+                        "sinonimi/termini collegati": ["" for _ in cols],
+                        "livello": ["comunale" for _ in cols],
+                    })
+                    print(f"OK. Catalogo variabili generato dal dataset principale ({len(cols)} colonne)")
+                    return df
+                except Exception as e:
+                    print(f"ERRORE durante la lettura del dataset per il catalogo: {e}")
 
         # Fallback minimo (ultima spiaggia)
-        df = pd.DataFrame([{
-        "variabile": "pop_totale",
-        "descrizione": "Popolazione residente",
-        "sinonimi/termini collegati": "popolazione, abitanti, residenti",
-        "livello": "comunale",
-     }])
-        print("�s���? Nessun dizionario trovato, uso fallback minimo (solo pop_totale).")
+        df = pd.DataFrame([
+            {
+                "variabile": "pop_totale",
+                "descrizione": "Popolazione residente",
+                "sinonimi/termini collegati": "popolazione, abitanti, residenti",
+                "livello": "comunale",
+            }
+        ])
+        print("Nessun dizionario trovato, uso fallback minimo (solo pop_totale).")
         return df
*** End Patch